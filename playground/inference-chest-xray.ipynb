{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3395b859-b718-4920-ba90-94eb4d243eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from VQUnet2D import Net as QNet\n",
    "from Unet2D import Net as Net\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5995e17e-8374-4319-bbae-cec40a71359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_classes=3)\n",
    "net.load_state_dict(torch.load('/vol/biomedic2/agk21/PhDLogs/codes/Vector-Quantisation-for-Robust-Segmentation/Logs/outputchestsegfinal/outputchestsegfinal/unetNIH/best_metric_model-v1.ckpt')['state_dict'])\n",
    "net = net.cuda()\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f724dd-b5ad-40b8-8d3e-87ae03ffbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Unet2D import NIHDataLoader\n",
    "loader = NIHDataLoader(\n",
    "        batch_size=1,\n",
    "        num_workers=4,\n",
    "        base_path=\"/vol/biodata/data/chest_xray/NIH/\",\n",
    "        csv_path=\"/vol/biomedic3/mmr12/data/chestXR/NIHCC-CXR14/\",\n",
    "        csv_names=[\"{}_NIH.csv\".format(_set) for _set in ['train', 'val', 'test']],\n",
    "        input_channels=1\n",
    "    )\n",
    "\n",
    "val_dataloader = loader.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5552ee-b2bd-4a4c-b652-119a5cef76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy(noise_typ, image, weightage = 0.0):\n",
    "    if noise_typ == \"gauss\":\n",
    "        mean = 0\n",
    "        var = 1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma,image.shape)\n",
    "        gauss = gauss.reshape(*image.shape)\n",
    "        noisy = image + weightage*gauss\n",
    "        return noisy\n",
    "    elif noise_typ == \"s&p\":\n",
    "        s_vs_p = 0.5\n",
    "        amount = weightage\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p) + 1\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape[-2:]]\n",
    "        for x, y in zip(*coords):\n",
    "            out[0, 0, x, y] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p)) + 1\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape[-2:]]\n",
    "        for x, y in zip(*coords):\n",
    "            out[0, 0, x, y] = 0\n",
    "        return out\n",
    "    \n",
    "    elif noise_typ == \"poisson\":\n",
    "        poisson_noise = np.sqrt(np.abs(image)) * np.random.normal(0, 1, image.shape)\n",
    "        noisy = image + weightage* poisson_noise\n",
    "        return noisy\n",
    "    \n",
    "    elif noise_typ ==\"speckle\":\n",
    "        gauss = np.random.randn(*image.shape)\n",
    "        gauss = gauss.reshape(*image.shape)        \n",
    "        noisy = image + weightage*image * gauss\n",
    "        return noisy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edde111b-30b7-4229-a7b9-063f8140ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def generate_TTA_results(name, data, noise_type='gauss', base_path='.'):\n",
    "    nTTAs = 25\n",
    "    zdata = {}\n",
    "    \n",
    "    assert noise_type in ['gauss', 's&p', 'poisson', 'speckle'], 'invalid noise_type'\n",
    "    \n",
    "    noise_threshold_list = [0.0, 0.01, 0.1, 0.5, 0.75]\n",
    "    \n",
    "    for noise_threshold in noise_threshold_list:\n",
    "        embs = []; recons = []\n",
    "        for _ in range(nTTAs):\n",
    "            x = data[0][:1, ...].numpy()\n",
    "            dtype = data[0].dtype\n",
    "            x = noisy(noise_type, x, noise_threshold)\n",
    "            x = torch.tensor(x).type(dtype).cuda() \n",
    "            with torch.no_grad():\n",
    "                x, encoding = net._model(x)\n",
    "            embs.append(encoding)\n",
    "            recons.append(x)\n",
    "\n",
    "        embs = torch.cat(embs, 0)\n",
    "        recons = torch.cat(recons, 0)\n",
    "        recons = torch.argmax(recons, 1)\n",
    "        \n",
    "        zdata[noise_threshold] = {'emb': embs.cpu().numpy(), \n",
    "                                      'recon': recons.cpu().squeeze().numpy(), \n",
    "                                      'img': data[0][:1, ...].squeeze().numpy(), \n",
    "                                      'label': data[1][:1, ...].squeeze().numpy()}\n",
    "\n",
    "    base_path = os.path.join(base_path, noise_type)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    with open(os.path.join(base_path,'{}.pickle'.format(name)), 'wb') as file:\n",
    "        pickle.dump(zdata, file)\n",
    "    return zdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a488e-94b1-4079-83b0-5698a05a070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████████████▎                                                                                                                         | 23/100 [25:25<1:33:33, 72.91s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_zdata = {}\n",
    "for i in tqdm(range(100)):\n",
    "    data = next(iter(val_dataloader))\n",
    "    all_zdata[i] = {}\n",
    "    all_zdata[i]['gauss'] = generate_TTA_results('image{}'.format(i), data, 'gauss', 'CXray/UNet')\n",
    "    all_zdata[i]['s&p'] = generate_TTA_results('image{}'.format(i), data, 's&p', 'CXray/UNet')\n",
    "    all_zdata[i]['poisson'] = generate_TTA_results('image{}'.format(i), data, 'poisson', 'CXray/UNet')\n",
    "    all_zdata[i]['speckle'] = generate_TTA_results('image{}'.format(i), data, 'speckle', 'CXray/UNet')\n",
    "    \n",
    "with open('all_data_CXray_UNet.pickle', 'wb') as file:\n",
    "        pickle.dump(all_zdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3db69-70c6-473d-b8bb-be4812f5d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].device, net.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8481e5-4b1b-4ec3-94ef-70e4ca51db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea988b5-aee0-417b-ac1c-7cffc36fa624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbcf6f-0e6f-410b-998e-89155f4717f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnet = QNet(num_classes=3)\n",
    "qnet.load_state_dict(torch.load('/vol/biomedic2/agk21/PhDLogs/codes/Vector-Quantisation-for-Robust-Segmentation/Logs/outputchestsegfinal/outputchestsegfinal/vqNIH/best_metric_model-v1.ckpt')['state_dict'])\n",
    "qnet = qnet.eval()\n",
    "qnet = qnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43af19-1eca-4276-b8bd-ec4fe4756336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def VQgenerate_TTA_results(name, data, noise_type='gauss', base_path='.'):\n",
    "    nTTAs = 25\n",
    "    zdata = {}\n",
    "    \n",
    "    assert noise_type in ['gauss', 's&p', 'poisson', 'speckle'], 'invalid noise_type'\n",
    "    \n",
    "    noise_threshold_list = [0.0, 0.01, 0.1, 0.5, 0.75]\n",
    "    \n",
    "    for noise_threshold in noise_threshold_list:\n",
    "        embs = []; recons = []; emb_losses = []\n",
    "        for _ in range(nTTAs):\n",
    "            x = data[0][:1, ...].numpy()\n",
    "            dtype = data[0].dtype\n",
    "            x = noisy(noise_type, x, noise_threshold)\n",
    "            x = torch.tensor(x).type(dtype).cuda() \n",
    "            with torch.no_grad():\n",
    "                x, emb_loss, encoding = qnet._model(x)\n",
    "            embs.append(encoding)\n",
    "            recons.append(x)\n",
    "            emb_losses.append(emb_loss.unsqueeze(0))\n",
    "\n",
    "        embs = torch.cat(embs, 0)\n",
    "        emb_losses = torch.cat(emb_losses, 0)\n",
    "        recons = torch.cat(recons, 0)\n",
    "        recons = torch.argmax(recons, 1)\n",
    "        \n",
    "        zdata[noise_threshold] = {'emb': embs.detach().cpu().numpy(),\n",
    "                                      'emb_loss': emb_losses.detach().cpu().numpy(),\n",
    "                                      'recon': recons.detach().cpu().squeeze().numpy(), \n",
    "                                      'img': data[0][:1, ...].squeeze().numpy(), \n",
    "                                      'label': data[1][:1, ...].squeeze().numpy()}\n",
    "\n",
    "    base_path = os.path.join(base_path, noise_type)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    with open(os.path.join(base_path,'{}.pickle'.format(name)), 'wb') as file:\n",
    "        pickle.dump(zdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52b9fd-cbd6-4f01-be96-27574f2c6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_zqdata = {}\n",
    "for i in tqdm(range(100)):\n",
    "    data = next(iter(val_dataloader))\n",
    "    all_zqdata[i] = {}\n",
    "    all_zqdata[i]['gauss'] = VQgenerate_TTA_results('image{}'.format(i), data, 'gauss', 'CXray/VQNet')\n",
    "    all_zqdata[i]['s&p'] = VQgenerate_TTA_results('image{}'.format(i), data, 's&p', 'CXray/VQNet')\n",
    "    all_zqdata[i]['poisson'] = VQgenerate_TTA_results('image{}'.format(i), data, 'poisson', 'CXray/VQNet')\n",
    "    all_zqdata[i]['speckle'] = VQgenerate_TTA_results('image{}'.format(i), data, 'speckle', 'CXray/VQNet')\n",
    "    \n",
    "with open('all_data_CXray_VQNet.pickle', 'wb') as file:\n",
    "        pickle.dump(all_zqdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae266166-a47d-434d-99f6-ae36cdc01c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17b926-ac47-424a-a506-0619d56d5059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf923e-9566-4211-af01-6d0e8077b4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7429f98-531a-462b-bb26-47fe93097dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d2d64-3b44-413f-9ce2-9508be7c1808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d12de6-4078-45e3-895d-611f1f861e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e05c5-e26f-4bb8-bc36-3be6efa3cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dice = lambda p, l: np.mean([2*(p == i)*(l == i)/((p==i) + (l ==i) + 1e-3) for i in np.unique(l)])\n",
    "import pandas as pd\n",
    "\n",
    "def _draw_(dict_info, noise_type='gauss', base_dir= '.'):\n",
    "    nimgs = len(dict_info.keys())\n",
    "    test_ = dict_info[0][noise_type]\n",
    "    noise_threshold_list = list(test_.keys())\n",
    "    \n",
    "    test_embs = test_[noise_threshold_list[0]]['emb'] # nttas, ...\n",
    "    nttas = test_embs.shape[0]\n",
    "    z_dim = np.prod(test_embs.shape[1:])\n",
    "    \n",
    "    \n",
    "    dice_scores_x = []\n",
    "    dice_scores_y = []\n",
    "    categories = []\n",
    "    \n",
    "    for ni, noise_threshold in enumerate(noise_threshold_list):\n",
    "        \n",
    "            \n",
    "        result_img = np.zeros((nimgs, z_dim))\n",
    "        \n",
    "        for iimg in range(nimgs):\n",
    "            z = dict_info[iimg][noise_type][noise_threshold]['emb'].reshape(nttas, -1)\n",
    "            z = (z - z.min())/(z.max() - z.min())\n",
    "            \n",
    "            result_img[iimgs, :] = np.var(z, axis=0).T\n",
    "            \n",
    "            if noise_threshold == 0.0:\n",
    "                dice_scores_x.append(np.mean([mean_dice(dict_info[iimg][noise_type][noise_threshold]['recon'][itta, ...], \n",
    "                                                    dict_info[iimg][noise_type][noise_threshold]['label']) \\\n",
    "                                                      for itta in range(nttas)]))\n",
    "            else:\n",
    "                dice_scores_y.append(np.mean([mean_dice(dict_info[iimg][noise_type][noise_threshold]['recon'][itta, ...], \n",
    "                                                    dict_info[iimg][noise_type][noise_threshold]['label']) \\\n",
    "                                                      for itta in range(nttas)]))\n",
    "                \n",
    "                categories.append('Noise Threshold: {}'.format(noise_threshold))\n",
    "                \n",
    "        \n",
    "        base_path = os.path.join(base_path, noise_type)\n",
    "        base_path = os.path.join(base_path, 'plots')\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        \n",
    "        plt.figure(figsize=(1, 10))\n",
    "        plt.imshow(result_img, cmap='coolwarm', vmin=0, vmax=1)\n",
    "        plt.axis (‘off’)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_path, 'NoiseT{}_NoiseType{}.png'.format(noise_threshold, noise_type)))\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['actual-dice'] = dice_scores_x * (len(noise_threshold_list) - 2)\n",
    "    df['perturbed- dice'] = dice_scores_y\n",
    "    df['categories'] = categories\n",
    "\n",
    "    p = sns.jointplot(data=df, x='actual-dice', y='perturbed- dice',  hue=\"categories\", alpha=0.5)\n",
    "    p.plot_joint(sns.kdeplot, levels=20)\n",
    "    p.fig.suptitle(\"aligned latent vectors\")\n",
    "    p.fig.tight_layout()\n",
    "    p.fig.subplots_adjust(top=0.95)\n",
    "    plt.savefig(os.path.join(base_path, 'dice_score_distribution_{}.png'.format(noise_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a10ad-3fcc-4f38-9618-ccb14397e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_data_CXray_UNet.pickle', 'rb') as file:\n",
    "    dict_info = pickle.load(file)\n",
    "    \n",
    "_draw_(dict_info, 'gauss', 'CXray/VQNet')\n",
    "_draw_(dict_info, 's&p', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'poisson', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'speckle', 'CXray/VQNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c50b2-0836-4361-8e04-425767cb34fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_data_CXray_VQNet.pickle', 'rb') as file:\n",
    "    dict_info = pickle.load(file)\n",
    "    \n",
    "_draw_(dict_info, 'gauss', 'CXray/VQNet')\n",
    "_draw_(dict_info, 's&p', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'poisson', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'speckle', 'CXray/VQNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
