{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615e920f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import VQUNet, VQtransUNet\n",
    "from trainer import Train\n",
    "from skimage.util import random_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459e1517",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(num_classes=3)\n",
    "net.load_state_dict(torch.load('/vol/biomedic2/agk21/PhDLogs/codes/Vector-Quantisation-for-Robust-Segmentation/Logs/outputchestsegfinal/outputchestsegfinal/unetNIH/best_metric_model-v1.ckpt')['state_dict'])\n",
    "net = net.cuda()\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2846e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataloaders import ChestXrayDataModule\n",
    "loader = ChestXrayDataModule(\n",
    "            batch_size=1,\n",
    "            num_workers=4,\n",
    "            binarise = False,\n",
    "            csv_train_img=/VQRobustSegmentation/data/Chest/train.csv',\n",
    "            csv_val_img=/VQRobustSegmentation/data/Chest/validation.csv',\n",
    "            csv_test_img=/VQRobustSegmentation/data/Chest/test.csv',\n",
    "        )\n",
    "\n",
    "val_dataloader = loader.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc08e34",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noisy(noise_typ, image, weightage = 0.0):\n",
    "    if noise_typ == \"gauss\":\n",
    "        mean = 0\n",
    "        var = 1\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma,image.shape)\n",
    "        gauss = gauss.reshape(*image.shape)\n",
    "        noisy = image + weightage*gauss\n",
    "        return noisy\n",
    "    elif noise_typ == \"s&p\":\n",
    "        s_vs_p = 0.5\n",
    "        amount = weightage\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p) + 1\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape[-2:]]\n",
    "        for x, y in zip(*coords):\n",
    "            out[0, 0, x, y] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p)) + 1\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape[-2:]]\n",
    "        for x, y in zip(*coords):\n",
    "            out[0, 0, x, y] = 0\n",
    "        return out\n",
    "    \n",
    "    elif noise_typ == \"poisson\":\n",
    "        poisson_noise = np.sqrt(np.abs(image)) * np.random.normal(0, 1, image.shape)\n",
    "        noisy = image + weightage* poisson_noise\n",
    "        return noisy\n",
    "    \n",
    "    elif noise_typ ==\"speckle\":\n",
    "        gauss = np.random.randn(*image.shape)\n",
    "        gauss = gauss.reshape(*image.shape)        \n",
    "        noisy = image + weightage*image * gauss\n",
    "        return noisy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b5747f0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def generate_TTA_results(name, data, noise_type='gauss', base_path='.'):\n",
    "    nTTAs = 25\n",
    "    zdata = {}\n",
    "    \n",
    "    assert noise_type in ['gauss', 's&p', 'poisson', 'speckle'], 'invalid noise_type'\n",
    "    \n",
    "    noise_threshold_list = [0.0, 0.01, 0.1, 0.5, 0.75]\n",
    "    \n",
    "    for noise_threshold in noise_threshold_list:\n",
    "        embs = []; recons = []\n",
    "        for _ in range(nTTAs):\n",
    "            x = data[0][:1, ...].numpy()\n",
    "            dtype = data[0].dtype\n",
    "            x = noisy(noise_type, x, noise_threshold)\n",
    "            x = torch.tensor(x).type(dtype).cuda() \n",
    "            with torch.no_grad():\n",
    "                x, encoding = net._model(x)\n",
    "            embs.append(encoding)\n",
    "            recons.append(x)\n",
    "\n",
    "        embs = torch.cat(embs, 0)\n",
    "        recons = torch.cat(recons, 0)\n",
    "        recons = torch.argmax(recons, 1)\n",
    "        \n",
    "        zdata[noise_threshold] = {'emb': embs.cpu().numpy(), \n",
    "                                      'recon': recons.cpu().squeeze().numpy(), \n",
    "                                      'img': data[0][:1, ...].squeeze().numpy(), \n",
    "                                      'label': data[1][:1, ...].squeeze().numpy()}\n",
    "\n",
    "    base_path = os.path.join(base_path, noise_type)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    with open(os.path.join(base_path,'{}.pickle'.format(name)), 'wb') as file:\n",
    "        pickle.dump(zdata, file)\n",
    "    return zdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60318e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|████████████████████████████████████▎                                                                                                                         | 23/100 [25:25<1:33:33, 72.91s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_zdata = {}\n",
    "for i in tqdm(range(100)):\n",
    "    data = next(iter(val_dataloader))\n",
    "    all_zdata[i] = {}\n",
    "    all_zdata[i]['gauss'] = generate_TTA_results('image{}'.format(i), data, 'gauss', 'CXray/UNet')\n",
    "    all_zdata[i]['s&p'] = generate_TTA_results('image{}'.format(i), data, 's&p', 'CXray/UNet')\n",
    "    all_zdata[i]['poisson'] = generate_TTA_results('image{}'.format(i), data, 'poisson', 'CXray/UNet')\n",
    "    all_zdata[i]['speckle'] = generate_TTA_results('image{}'.format(i), data, 'speckle', 'CXray/UNet')\n",
    "    \n",
    "with open('all_data_CXray_UNet.pickle', 'wb') as file:\n",
    "        pickle.dump(all_zdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588480f7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[0].device, net.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f184fa6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ddb5a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7dd2b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffb21a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def VQgenerate_TTA_results(name, data, noise_type='gauss', base_path='.'):\n",
    "    nTTAs = 25\n",
    "    zdata = {}\n",
    "    \n",
    "    assert noise_type in ['gauss', 's&p', 'poisson', 'speckle'], 'invalid noise_type'\n",
    "    \n",
    "    noise_threshold_list = [0.0, 0.01, 0.1, 0.5, 0.75]\n",
    "    \n",
    "    for noise_threshold in noise_threshold_list:\n",
    "        embs = []; recons = []; emb_losses = []\n",
    "        for _ in range(nTTAs):\n",
    "            x = data[0][:1, ...].numpy()\n",
    "            dtype = data[0].dtype\n",
    "            x = noisy(noise_type, x, noise_threshold)\n",
    "            x = torch.tensor(x).type(dtype).cuda() \n",
    "            with torch.no_grad():\n",
    "                x, emb_loss, encoding, info = qnet._model(x)\n",
    "            embs.append(encoding)\n",
    "            recons.append(x)\n",
    "            emb_losses.append(emb_loss.unsqueeze(0))\n",
    "\n",
    "        embs = torch.cat(embs, 0)\n",
    "        emb_losses = torch.cat(emb_losses, 0)\n",
    "        recons = torch.cat(recons, 0)\n",
    "        recons = torch.argmax(recons, 1)\n",
    "        \n",
    "        zdata[noise_threshold] = {'emb': embs.detach().cpu().numpy(),\n",
    "                                      'emb_loss': emb_losses.detach().cpu().numpy(),\n",
    "                                      'recon': recons.detach().cpu().squeeze().numpy(), \n",
    "                                      'img': data[0][:1, ...].squeeze().numpy(), \n",
    "                                      'label': data[1][:1, ...].squeeze().numpy()}\n",
    "\n",
    "    base_path = os.path.join(base_path, noise_type)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    with open(os.path.join(base_path,'{}.pickle'.format(name)), 'wb') as file:\n",
    "        pickle.dump(zdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a840a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_zqdata = {}\n",
    "for i in tqdm(range(100)):\n",
    "    data = next(iter(val_dataloader))\n",
    "    all_zqdata[i] = {}\n",
    "    all_zqdata[i]['gauss'] = VQgenerate_TTA_results('image{}'.format(i), data, 'gauss', 'CXray/VQNet')\n",
    "    all_zqdata[i]['s&p'] = VQgenerate_TTA_results('image{}'.format(i), data, 's&p', 'CXray/VQNet')\n",
    "    all_zqdata[i]['poisson'] = VQgenerate_TTA_results('image{}'.format(i), data, 'poisson', 'CXray/VQNet')\n",
    "    all_zqdata[i]['speckle'] = VQgenerate_TTA_results('image{}'.format(i), data, 'speckle', 'CXray/VQNet')\n",
    "    \n",
    "with open('all_data_CXray_VQNet.pickle', 'wb') as file:\n",
    "        pickle.dump(all_zqdata, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0af233",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24721b77",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205daca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cc9ae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd75e7c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837ea1f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9330a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_dice = lambda p, l: np.mean([2*(p == i)*(l == i)/((p==i) + (l ==i) + 1e-3) for i in np.unique(l)])\n",
    "import pandas as pd\n",
    "\n",
    "def _draw_(dict_info, noise_type='gauss', base_dir= '.'):\n",
    "    nimgs = len(dict_info.keys())\n",
    "    test_ = dict_info[0][noise_type]\n",
    "    noise_threshold_list = list(test_.keys())\n",
    "    \n",
    "    test_embs = test_[noise_threshold_list[0]]['emb'] # nttas, ...\n",
    "    nttas = test_embs.shape[0]\n",
    "    z_dim = np.prod(test_embs.shape[1:])\n",
    "    \n",
    "    \n",
    "    dice_scores_x = []\n",
    "    dice_scores_y = []\n",
    "    categories = []\n",
    "    \n",
    "    for ni, noise_threshold in enumerate(noise_threshold_list):\n",
    "        \n",
    "            \n",
    "        result_img = np.zeros((nimgs, z_dim))\n",
    "        \n",
    "        for iimg in range(nimgs):\n",
    "            z = dict_info[iimg][noise_type][noise_threshold]['emb'].reshape(nttas, -1)\n",
    "            z = (z - z.min())/(z.max() - z.min())\n",
    "            \n",
    "            result_img[iimgs, :] = np.var(z, axis=0).T\n",
    "            \n",
    "            if noise_threshold == 0.0:\n",
    "                dice_scores_x.append(np.mean([mean_dice(dict_info[iimg][noise_type][noise_threshold]['recon'][itta, ...], \n",
    "                                                    dict_info[iimg][noise_type][noise_threshold]['label']) \\\n",
    "                                                      for itta in range(nttas)]))\n",
    "            else:\n",
    "                dice_scores_y.append(np.mean([mean_dice(dict_info[iimg][noise_type][noise_threshold]['recon'][itta, ...], \n",
    "                                                    dict_info[iimg][noise_type][noise_threshold]['label']) \\\n",
    "                                                      for itta in range(nttas)]))\n",
    "                \n",
    "                categories.append('Noise Threshold: {}'.format(noise_threshold))\n",
    "                \n",
    "        \n",
    "        base_path = os.path.join(base_path, noise_type)\n",
    "        base_path = os.path.join(base_path, 'plots')\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        \n",
    "        plt.figure(figsize=(1, 10))\n",
    "        plt.imshow(result_img, cmap='coolwarm', vmin=0, vmax=1)\n",
    "        plt.axis (‘off’)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_path, 'NoiseT{}_NoiseType{}.png'.format(noise_threshold, noise_type)))\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame()\n",
    "    df['actual-dice'] = dice_scores_x * (len(noise_threshold_list) - 2)\n",
    "    df['perturbed- dice'] = dice_scores_y\n",
    "    df['categories'] = categories\n",
    "\n",
    "    p = sns.jointplot(data=df, x='actual-dice', y='perturbed- dice',  hue=\"categories\", alpha=0.5)\n",
    "    p.plot_joint(sns.kdeplot, levels=20)\n",
    "    p.fig.suptitle(\"aligned latent vectors\")\n",
    "    p.fig.tight_layout()\n",
    "    p.fig.subplots_adjust(top=0.95)\n",
    "    plt.savefig(os.path.join(base_path, 'dice_score_distribution_{}.png'.format(noise_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2fe4aa",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_data_CXray_UNet.pickle', 'rb') as file:\n",
    "    dict_info = pickle.load(file)\n",
    "    \n",
    "_draw_(dict_info, 'gauss', 'CXray/VQNet')\n",
    "_draw_(dict_info, 's&p', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'poisson', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'speckle', 'CXray/VQNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50997912",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('all_data_CXray_VQNet.pickle', 'rb') as file:\n",
    "    dict_info = pickle.load(file)\n",
    "    \n",
    "_draw_(dict_info, 'gauss', 'CXray/VQNet')\n",
    "_draw_(dict_info, 's&p', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'poisson', 'CXray/VQNet')\n",
    "_draw_(dict_info, 'speckle', 'CXray/VQNet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
